# -*- coding: utf-8 -*-
"""Sq2Sq.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16NmjnvftqlEHWN2LRKVm6_FvHudR1IZA
"""

!ls

import numpy as np
import torch
from torch.nn import *
from torch.nn.functional import *
from torchtext.datasets import Multi30k
from torchtext.data import Field,BucketIterator
import spacy
import random
import torch.optim as opt
device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

eng=spacy.load('en')
ger=spacy.load('de_core_news_sm')

def Tokenize_eng(text):
  return [a.text for a in eng.tokenizer(text)]
def Tokenize_german(text):
  return [b.text for b in ger.tokenizer(text)]

german=Field(tokenize=Tokenize_german,lower=True,init_token='<sos>',eos_token='<eos>')
english=Field(tokenize=Tokenize_eng,lower=True,init_token='<sos>',eos_token='<eos>')

Train,Val,Test=Multi30k.splits(exts=('.de','.en'),fields=(german,english))

german.build_vocab(Train,max_size=10000,min_freq=2)
english.build_vocab(Train,max_size=10000,min_freq=2)

##building encoder
class Encode(Module):
  def __init__(self,inp_size,emd_size,hidden_size):
    super(Encode,self).__init__()
    self.inp_size=inp_size
    self.emd_size=emd_size
    self.hidden_size=hidden_size
    self.embed=Embedding(self.inp_size,self.emd_size)
    self.lstm=LSTM(self.emd_size,self.hidden_size,num_layers=2,dropout=0.3)
  def forward(self,x):
    x=self.embed(x)
    x,(h,c)=self.lstm(x)
    return h,c

class Decoder(Module):
  def __init__(self,vocab,embed_dim,hidden_dim,output_dim):
    super(Decoder,self).__init__()
    self.embed_dim=embed_dim
    self.hidden_dim=hidden_dim
    self.output_dim=output_dim
    self.embed=Embedding(vocab,self.embed_dim)
    self.lstm=LSTM(self.embed_dim,self.hidden_dim,num_layers=2,dropout=0.2)
    self.l1=Linear(self.hidden_dim,64)
    self.l2=Linear(64,256)
    self.l3=Linear(256,self.output_dim)
  def forward(self,x,h,c):
    x=x.unsqueeze(0)
    x=self.embed(x)
    x,(h,c)=self.lstm(x,(h,c))
    x=relu(self.l1(x))
    x=relu(self.l2(x))
    x=self.l3(x)
    x=x.squeeze(0) 
    return x,h,c

class Encoder_Decoder(Module):
  def __init__(self,en,dec):
    super(Encoder_Decoder,self).__init__()
    self.encoder=en;
    self.decoder=dec
  def forward(self,inp,target,ratio=0.5):
    batch_size=inp.shape[1]
    seq_len=target.shape[0]
    vocab=len(english.vocab)
    outputs=torch.zeros((seq_len,batch_size,vocab)).to(device)
    h,c=self.encoder(inp)
    x=target[0]
    #autoregression / teacher forcing
    for t in range(1,seq_len):
      output,h,c=self.decoder(x,h,c)
      outputs[t]=output
      best_guess=output.argmax(1)
      x=target[t] if random.random()<ratio else best_guess
    return outputs

epochs=200
encoder_vocab=len(german.vocab)
decoder_vocab=len(english.vocab)
embed_encoder=300
embed_decoder=300

TrainD,ValD,TestD=BucketIterator.splits((Train,Val,Test),batch_size=100,sort_within_batch=True,sort_key=lambda x:len(x.src),device=device)

for a,b in TrainD:
  print(a[0].shape,a[1].shape)

encoder_net=Encode(encoder_vocab,embed_encoder,256).to(device)
decoder_net=Decoder(decoder_vocab,embed_decoder,256,decoder_vocab).to(device)
Sequence_net=Encoder_Decoder(encoder_net,decoder_net).to(device)

pad_index=english.vocab.stoi['<pad>']
optimizer=opt.Adam(Sequence_net.parameters(),lr=0.001)
loss_f=CrossEntropyLoss(ignore_index=pad_index)

for i in range(epochs):
  L=0
  for batch in TrainD:
    inp=batch.src.to(device)
    targ=batch.trg.to(device)
    output=Sequence_net(inp,targ)
    output=output[1:]
    output=output.reshape(-1,output.shape[-1])
    targ=targ[1:].reshape(-1)
    optimizer.zero_grad()
    loss=loss_f(output,targ)
    loss.backward()
    L+=loss.item()
    torch.nn.utils.clip_grad_norm(Sequence_net.parameters(),max_norm=1)
    optimizer.step()
  print(f"Epoch{i+1} has loss {L}| unkown");



torch.save(Sequence_net,'/content/lol.pt')

import pickle
with open('/content/german.pickle','wb') as file:
  pickle.dump(german,file)

